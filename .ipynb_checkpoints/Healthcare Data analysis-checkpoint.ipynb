{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import json\n",
    "import ijson\n",
    "from pandas.io.json import json_normalize\n",
    "from flatten_json import flatten as flatten_json\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "#Flatten array of structs and structs\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful Functions \n",
    "\n",
    "#Flatten function\n",
    "def pyspark_flatten(df):\n",
    "   # compute Complex Fields (Lists and Structs) in Schema   \n",
    "   complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   while len(complex_fields)!=0:\n",
    "      col_name=list(complex_fields.keys())[0]\n",
    "      print (\"Processing :\"+col_name+\" Type : \"+str(type(complex_fields[col_name])))\n",
    "    \n",
    "      # if StructType then convert all sub element to columns.\n",
    "      # i.e. flatten structs\n",
    "      if (type(complex_fields[col_name]) == StructType):\n",
    "         expanded = [col(col_name+'.'+k).alias(col_name+'_'+k) for k in [ n.name for n in  complex_fields[col_name]]]\n",
    "         df=df.select(\"*\", *expanded).drop(col_name)\n",
    "    \n",
    "      # if ArrayType then add the Array Elements as Rows using the explode function\n",
    "      # i.e. explode Arrays\n",
    "      elif (type(complex_fields[col_name]) == ArrayType):    \n",
    "         df=df.withColumn(col_name,explode_outer(col_name))\n",
    "    \n",
    "      # recompute remaining Complex Fields in Schema       \n",
    "      complex_fields = dict([(field.name, field.dataType)\n",
    "                             for field in df.schema.fields\n",
    "                             if type(field.dataType) == ArrayType or  type(field.dataType) == StructType])\n",
    "   return df\n",
    "\n",
    "\n",
    "# Outputs the columns that need to be dropped\n",
    "def to_drop(dataset, corr_threshold, useless_columns):\n",
    "    # Create correlation matrix\n",
    "    corr_matrix = dataset.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    # Find index of feature columns with correlation greater than corr_threshold\n",
    "    drop = [column for column in upper.columns if any(upper[column] > corr_threshold)]\n",
    "    drop = drop + useless_columns\n",
    "    return drop\n",
    "\n",
    "# Drops the outliers from the dataset \n",
    "def drop_outliers(dataset):\n",
    "    for column in dataset._get_numeric_data():\n",
    "        data_column = dataset[column]\n",
    "        outliers = data_column[((data_column - data_column.mean()) / data_column.std()).abs() > 3]\n",
    "        if (len(outliers) < 20) and (len(outliers) != 0):\n",
    "            dataset[column] = data_column[((data_column - data_column.mean()) / data_column.std()).abs() < 3]\n",
    "            \n",
    "# Outputs data ready for use\n",
    "def clean_data (dataset, drop_columns):\n",
    "    new_data = dataset.drop(columns=drop_columns)\n",
    "    drop_outliers(new_data)\n",
    "#     new_data = new_data.dropna()\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appName = \"HealthcareSQL\"\n",
    "# master = \"local\"\n",
    "\n",
    "# sc = SparkContext()\n",
    "\n",
    "# # Create Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(appName) \\\n",
    "#     .master(master) \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.7:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fe7a761c310>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"drug-event-0039-of-0039.json\"\n",
    "\n",
    "# Option 1 \n",
    "with open(filename, 'r') as json_file:\n",
    "    data_json = ijson.items(json_file, 'results.item')\n",
    "    items = list(data_json)\n",
    "\n",
    "# data_1 = json_normalize(data = items, record_path =[['patient', 'reaction'], 'drug'] , sep='_')\n",
    "data_1 = json_normalize(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Option 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_extract = {\n",
    "    'companynumb',\n",
    "    'safetyreportid',\n",
    "    'safetyreportversion',\n",
    "    'receiptdate',\n",
    "    'patientagegroup',\n",
    "    'patientdeathdate',\n",
    "    'patientsex',\n",
    "    'patientweight',\n",
    "    'serious',\n",
    "    'seriousnesscongenitalanomali',\n",
    "    'seriousnessdeath',\n",
    "    'seriousnessdisabling',\n",
    "    'seriousnesshospitalization',\n",
    "    'seriousnesslifethreatening',\n",
    "    'seriousnessother',\n",
    "    'actiondrug',\n",
    "    'activesubstancename',\n",
    "    'drugadditional',\n",
    "    'drugadministrationroute',\n",
    "    'drugcharacterization',\n",
    "    'drugindication',\n",
    "    'drugauthorizationnumb',\n",
    "    'medicinalproduct',\n",
    "    'drugdosageform',\n",
    "    'drugdosagetext',\n",
    "    'reactionoutcome',\n",
    "    'reactionmeddrapt',\n",
    "    'reactionmeddraversionpt'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_layer(dic, level):\n",
    "    layers = []\n",
    "    for key in dic:\n",
    "        if dic[key][1] == level:\n",
    "            if dic[key][2] == True:\n",
    "                layers.append(key)\n",
    "    return layers\n",
    "\n",
    "def create_layers(dic, parent, level):\n",
    "    layers = {}\n",
    "    \n",
    "    for key in dic.keys():\n",
    "        value = dic[key]\n",
    "        value_type = type(value)\n",
    "\n",
    "        if value_type == dict or value_type == list:\n",
    "            layers.update({key:(parent,level, True)})\n",
    "        elif value_type == str:\n",
    "            layers.update({key:(parent,level, False)})\n",
    "        else:\n",
    "            layers.update({key:(parent,level, False)})\n",
    "          \n",
    "    return layers\n",
    "\n",
    "def gen_dict_extract(key, var):\n",
    "    if hasattr(var,'items'):\n",
    "        for k, v in var.items():\n",
    "            if k == key:\n",
    "                yield v\n",
    "            if isinstance(v, dict):\n",
    "                for result in gen_dict_extract(key, v):\n",
    "                    yield result\n",
    "            elif isinstance(v, list):\n",
    "                for d in v:\n",
    "                    for result in gen_dict_extract(key, d):\n",
    "                        yield result\n",
    "\n",
    "def findkeys(node, kv):\n",
    "    if isinstance(node, list):\n",
    "        for i in node:\n",
    "            for x in findkeys(i, kv):\n",
    "                yield x\n",
    "    elif isinstance(node, dict):\n",
    "        if kv in node:\n",
    "            yield node[kv]\n",
    "        for j in node.values():\n",
    "            for x in findkeys(j, kv):\n",
    "                yield x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "formated_items = []\n",
    "for item in items:\n",
    "    new_dict = {}\n",
    "    for column in columns_to_extract:\n",
    "        column_iter_list = tuple(findkeys(item, column))\n",
    "        if len(column_iter_list) == 1:\n",
    "            new_dict.update({column:column_iter_list[0]})\n",
    "        else:\n",
    "            new_dict.update({column:column_iter_list})\n",
    "    formated_items.append(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(formated_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n",
      "Missing values: 0\n",
      "Single valued columns: Index(['patientdeathdate'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#checking number of duplicates, missing values and columns with a single value\n",
    "print(\"Duplicates:\",final_data.duplicated().sum())\n",
    "print(\"Missing values:\",final_data.isna().sum().sum())\n",
    "print(\"Single valued columns:\", final_data.columns[final_data.nunique()==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.drop(columns='patientdeathdate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('drug-event.csv')"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
